---
title: 8.1 Statistical Inference Continued Hypothesis Testing
subtitle: PLSC30500, Fall 2021
author: 
  # - co-taught by Molly Offer-Westort & Andy Eggers
  - .small-text[(This lecture with references to Aronow & Miller 2019)]
output: 
  xaringan::moon_reader:
    self_contained: true
    css: [default, uchicago_pol_meth.css]
    nature:
      highlightLines: true
      countIncrementalSlides: no
---

```{r setup, include=FALSE}
library(tidyverse)
set.seed(60637)
options(width = 60)
```

```{css, echo=FALSE}
.small-output .remark-code{
  font-size: x-small;
}

.white { color: white; }
.red { color: red; }
.blue { color: blue; }

# .show-only-last-code-result pre + pre:not(:last-of-type) code[class="remark-code"] {
#     display: none;
# }
```



- We have some data that are produced from an i.i.d. sampling procedure. 

--
- We've produced an estimate of some target estimand using our estimating procedure. 

--
- We then produced an estimate of the standard error of our estimate. 

--
- Now we would like to be able to say something what that means. 


---

# Confidence intervals

- A valid confidence interval $CI_n$ for a target parameter $\theta$ with coverage $1-\alpha$
$$
\textrm{P}[\theta \in CI_n]\ge 1- \alpha
$$
--

- $CI_n$ is a random interval. It is a function of the data we observe. 
--

- $\theta$ is a fixed parameter. It does not move. $^*$ 

--

( $^*$ In the frequentist view of statistics.)


- If you use valid confidence repeatedly in your work, 95% of the time, your confidence intervals will include the true value of the relevant $\theta.$

---

- We could trivially define valid confidence intervals by including the entire support of the data. 
--
(Why wouldn't we want to do that?)

---

## Normal approximation-based confidence intervals

Let $\hat\theta_n$ be an asymptotically normal estimator of some estimand $\theta$. Let $\hat{\textrm{se}}$ be a consistent estimator of the standard error of the estimate. 

--

Since $\hat\theta_n$ is asymptotically normal, we can discuss coverage in terms of the normal distribution. 


---

Recall the normal distribution. It has a bell curve shape, with more density around the middle, and less density at more extreme values. 


```{r, fig.width = 6, fig.height=5, fig.align = 'center', echo=FALSE}
result_n <- rnorm(n = 10000)
plotdata <- tibble(
  x = result_n,
  Fx = pnorm(result_n),
  fx = dnorm(result_n)
)

g <- ggplot(plotdata, aes(x = x, y = fx)) +
  geom_line() +
  coord_cartesian(xlim = c(-2.5, 2.5),
                  ylim = c(0,0.5)) +
  ggtitle('PDF of Standard Normal Distribution')

g +
  geom_vline(xintercept = 0, lty = 'dashed', color = 'skyblue') + 
  geom_segment(aes(x = 0, xend = -1, y = 0.2, yend = 0.2), 
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_segment(aes(x = 0, xend = 1, y = 0.2, yend = 0.2), 
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_point(aes(x = 0, y = 0.2), color = 'skyblue') + 
  annotate(geom="text", x = 0.5, y = .19, label = as.character(expression(sigma)), parse = TRUE, color = 'steelblue') + 
  annotate(geom="text", x = -0.5, y = .19, label = as.character(expression(sigma)), parse = TRUE, color = 'steelblue') + 
  annotate(geom="text", x = 0.075, y = .42, label = as.character(expression(mu)), parse = TRUE, color = 'steelblue')

```


---

For $0 \le c \le 1$,  $z(c)$ describes the $c$-th quantile of the normal distribution. 

--

For example, the 5th quantile describes the point which is greater than 5% of the distribution. 


```{r, fig.width = 6, fig.height=5, fig.align = 'center', echo=FALSE}
g +
  stat_function(fun = dnorm,
                geom = "area",
                fill = "skyblue",
                xlim = c(-10, qnorm(0.05))) +
  geom_vline(xintercept = qnorm(0.05), lty = 'dashed', color = 'skyblue') +  
  annotate(geom="text", x = qnorm(0.05), y = .2, label = round(qnorm(0.05), 3), parse = TRUE, color = 'steelblue')

```


---

For $0 \le c \le 1$,  $z(c)$ describes the $c$-th quantile of the normal distribution. 

--

In terms of the distribution of the standard normal distribution, $\Phi(z(c)) = c$, where $\Phi(\cdot)$ is the CDF.

--


The 95th quantile describes the point which is greater than 95% of the distribution. 


```{r, fig.width = 6, fig.height=5, fig.align = 'center', echo=FALSE}
g +
  stat_function(fun = dnorm,
                geom = "area",
                fill = "skyblue",
                xlim = c(-10, qnorm(0.95))) +
  geom_vline(xintercept = qnorm(0.95), lty = 'dashed', color = 'skyblue') +  
  annotate(geom="text", x = qnorm(0.95), y = .2, label = round(qnorm(0.95), 3), parse = TRUE, color = 'steelblue')


```


---

We can calculate $z(c)$ in R using the `qnorm()` function, which reports the value of quantile input. 

By default, it gives us the standard normal distribution, with mean 0 and sd 1. 

```{r}
qnorm(0.05)
qnorm(0.95)
```

--

Notice that for the standard normal distribution, with mean 0, the quantiles are symmetric around zero. 

---

If we want to describe symmetric bounds around the mean that contain 95% of the distribution, this would be from the 2.5th percentile to the 97.5th percentile. 

```{r, fig.width = 6, fig.height=5, fig.align = 'center', echo=FALSE}
g +
  stat_function(fun = dnorm,
                geom = "area",
                fill = "skyblue",
                xlim = c(qnorm(0.025), qnorm(0.975))) +
  geom_vline(xintercept = qnorm(0.975), lty = 'dashed', color = 'skyblue') + 
  geom_vline(xintercept = qnorm(0.025), lty = 'dashed', color = 'skyblue') +  
  annotate(geom="text", x = qnorm(0.025), y = .2, label = round(qnorm(0.025), 3), parse = TRUE, color = 'steelblue') +  
  annotate(geom="text", x = qnorm(0.975), y = .2, label = round(qnorm(0.975), 3), parse = TRUE, color = 'steelblue')


```

--

These values are about -2 and 2. We will use them a lot. 

---


Now, we can define the normal approximation-based confidence interval as:

$$
CI\_n = \left(\hat \theta\_n - z\_{1-\alpha/2} \times \hat{\textrm{se}},\  \theta\_n + z\_{1-\alpha/2}\times \hat{\textrm{se}} \right)
$$


--

For the 95% confidence interval, $\alpha = 0.05$ and, $z_{1-\alpha/2} \approx 1.96$.

$$
CI\_n = \left(\hat \theta\_n - 1.96 \times \hat{\textrm{se}},\  \theta\_n + 1.96 \times \hat{\textrm{se}} \right)
$$

--
Then,

$$
\textrm{P}[\theta \in CI_n] \rightarrow 1- \alpha.
$$

Asymptotically, the normal approximation-based confidence interval will have correct coverage. 

---

# [Example showing how the confidence interval moves around, while the true mean stays fixed]


---
## Applied example



We can see this in action with respect to a paper by Devah Pager: 


Pager, D. (2003). The mark of a criminal record. *American Journal of Sociology*, 108(5), 937-975.

---


```{r, message=FALSE}
mcr <- tibble(
  black = rep(c(0, 1), times = c(300, 400)),
  record = c(rep(c(0, 1), each = 150), 
             rep(c(0, 1), each = 200)),
  call_back = c(
    # whites without criminal records
    rep(c(0, 1), times = c(99, 51)), # 150
    # whites with criminal records
    rep(c(0, 1), times = c(125, 25)), # 150: could be 25 or 26 got callback
    # blacks without criminal records
    rep(c(0, 1), times = c(172, 28)), # 200
    # blacks with criminal records
    rep(c(0, 1), times = c(190, 10)) # 200 
  )
)

```






---

The study was an audit study, where pairs of white and pairs of black hypothetical job applicants applied to real jobs. 

--

The outcome is whether applicants got a callback. 


```{r, echo = FALSE}
mcr %>% 
  group_by(black, record) %>% 
  summarize(n = n(), 
            call_back = mean(call_back))
```


```{r pdb_fig, echo=FALSE, out.width = "20%", fig.align="center"}
knitr::include_graphics('assets/pager_2003.png')
```


---

Let's say our $\hat{\theta}$ right here is the overall mean of `call_back` among black applicants.

```{r}
(theta_hat <- mcr %>% 
   filter(black == 1) %>% 
   summarize(mean(call_back)) %>% 
   pull)
```
--

Our $\hat{se}$ is our estimate of the standard error of the mean, 
$$\hat{se} = \sqrt{\hat{\textrm{Var}}[X]/n};$$ 

we get this by plugging in our unbiased sample variance estimate into the formula for the standard error of the mean. 

```{r}
(se_hat <- mcr %>% 
   filter(black == 1) %>% 
   summarize(sqrt(var(call_back)/length(call_back))) %>% 
   pull)

```

---

We can then get our 95% confidence intervals by plugging into the formula, 


$$
CI\_n = \left(\hat \theta\_n - 1.96 \times \hat{\textrm{se}},\  \theta\_n + 1.96 \times \hat{\textrm{se}} \right)
$$


```{r}
(CI <- c(theta_hat + c(-1,1)*qnorm(1-0.025)*se_hat))
```




---

We can see the exact same confidence intervals in the output of `lm` or `estimatr::lm_robust`, if we use the `confint()` function with the `.default` version, which gives the asymptotically normal confidence intervals.

```{r}
library(estimatr)

model <- lm_robust(call_back ~1, data = mcr %>% filter(black == 1))

confint.default(model)
```

---

By default, `lm_robust` gives confidence intervals that are adjusted for the sample size--these are equivalent in large samples to our normal approximation estimators. 

--

To see exactly the same confidence intervals as in `lm_robust`, we can use the Student t distribution with the degrees of freedom, which we extract from the object. 

Degrees of freedom tell us how many observations we had in our sample, minus how many parameters we used in our model. 

```{r}
model
(dof <- model$df)

(CI <- c(theta_hat + c(-1,1)*qt(1-0.025, dof)*se_hat))
```



---
This is the normal distribution we're using as an *approximation* of the distribution of the sample mean, based on our estimates. .white[And our confidence intervals, overlaid on this distribution.]

```{r, fig.width = 6, fig.height=5, fig.align = 'center', echo=FALSE}
result_n <- rnorm(n = 10000, mean = theta_hat, sd = se_hat)
plotdata <- tibble(
  x = result_n,
  Fx = pnorm(result_n, mean = theta_hat, sd = se_hat),
  fx = dnorm(result_n, mean = theta_hat, sd = se_hat)
)

g <- ggplot(plotdata, aes(x = x, y = fx)) +
  geom_line() +
  coord_cartesian(xlim = c(qnorm(0.01, mean = theta_hat, sd = se_hat), 
                           qnorm(0.99, mean = theta_hat, sd = se_hat))) +
  ggtitle('Normal Approximation of the Distribution of the Sample Mean')

g +
  geom_vline(xintercept = theta_hat, lty = 'dashed', color = 'skyblue') + 
  geom_segment(aes(x = theta_hat, xend = theta_hat-se_hat, y = 10, yend = 10), 
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_segment(aes(x = theta_hat, xend = theta_hat + se_hat, y = 10, yend = 10), 
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_point(aes(x = theta_hat, y = 10), color = 'skyblue') + 
  annotate(geom="text", x = theta_hat - se_hat/2, y = 9, label = as.character(expression(sigma)), parse = TRUE, color = 'steelblue') + 
  annotate(geom="text", x = theta_hat + se_hat/2, y = 9, label = as.character(expression(sigma)), parse = TRUE, color = 'steelblue') + 
  annotate(geom="text", x = theta_hat + .005, y = 15, label = as.character(expression(mu)), parse = TRUE, color = 'steelblue')

```

---
This is the normal distribution we're using as an *approximation* of the distribution of the sample mean, based on our estimates. And our confidence intervals, overlaid on this distribution. 

```{r, fig.width = 6, fig.height=5, fig.align = 'center', echo=FALSE}
g +
  stat_function(fun = dnorm,
                args = list(mean = theta_hat, sd = se_hat),
                geom = "area",
                fill = "skyblue",
                xlim = c(qnorm(0.025, mean = theta_hat, sd = se_hat), 
                         qnorm(0.975, mean = theta_hat, sd = se_hat))) +
  geom_vline(xintercept = qnorm(0.975, mean = theta_hat, sd = se_hat), 
             lty = 'dashed', color = 'skyblue') + 
  geom_vline(xintercept = qnorm(0.025, mean = theta_hat, sd = se_hat), 
             lty = 'dashed', color = 'skyblue') +  
  annotate(geom="text", x = qnorm(0.025, mean = theta_hat, sd = se_hat), y = 10, 
           label = round(qnorm(0.025, mean = theta_hat, sd = se_hat), 3),
           parse = TRUE, color = 'steelblue') +  
  annotate(geom="text", x = qnorm(0.975, mean = theta_hat, sd = se_hat), y = 10, 
           label = round(qnorm(0.975, mean = theta_hat, sd = se_hat), 3), 
           parse = TRUE, color = 'steelblue')
```




---
# Hypothesis testing

In our randomization inference section, we specified the null distribution in terms of the individual treatment effect, $\tau_i$. 

--

When we use the normal approximation to conduct inference, we generally posit our null with respect to the *mean* of the parameter. 
--
I.e., 

$$
H_0: \tau = 0
$$

$$
H_A: \tau \neq 0
$$

---

## p-values

Suppose $\hat{\theta}$ is the general form for an estimate produced by our estimator, and $\hat{\theta}^*$ is the value we have actually observed. 

---

## p-values

- A lower one-tailed $p$-value under the null hypothesis is 

$$
p = \textrm{P}\_0[\hat{\theta} \le \hat{\theta}^*]
$$

i.e., the probability *under the null distribution* that we would see an estimate of $\hat{\theta}$ that is less than or equal to what we saw from the data. 



---

## p-values



- An upper one-tailed $p$-value under the null hypothesis is:

$$
p = \textrm{P}\_0[\hat{\theta} \ge \hat{\theta}^*]
$$

i.e., the probability *under the null distribution* that we would see an estimate of $\hat{\theta}$ that is greater than or equal to what we saw from the data. 

---

## p-values

- A two-tailed $p$-value under the null hypothesis is 

$$
p = \textrm{P}\_0[|\hat{\theta}| \ge |\hat{\theta}^*|]
$$

i.e., the probability *under the null distribution* that we would see an estimate of $\hat{\theta}$ as or more extreme as what we saw from the data. 




---

## T-statistic

How do we produce our $p$-values under the normal approximation based approach?

--

Suppose $\hat{\theta}$ is the general form for an estimate produced by our asymptotically normal estimator, and let $\hat{\theta}^*$ be the value of the estimate we have actually observed. 

Let $\hat{se}$ be a consistent estimator of the sampling variance of the estimator, such as, for the standard error of the sample mean, $\sqrt{S^2/n}$. 

--

The *t-statistic* is:
$$
t = \frac{\hat{\theta}^* - \theta_0}{\hat{se}}
$$

---
## Normal-approximation based p-values

We'll use the t-statistic to get our $p$-values. 

--

We plug the t-statistic produced by our estimates into the CDF function of the standard normal distribution, $\Phi(\cdot)$. 

--

- For an asymptotically valid lower one-tailed $p$-value **under the null**, 

$$
p = \Phi\left( \frac{\hat{\theta}^* - \theta_0}{\hat{se}} \right)
$$

--

- For an asymptotically valid upper one-tailed $p$-value **under the null**, 

$$
p = 1 - \Phi\left( \frac{\hat{\theta}^* - \theta_0}{\hat{se}} \right)
$$


--

- For an asymptotically valid two-tailed $p$-value **under the null**

$$
p = 2 \times \left( 1 - \Phi\left( \frac{\hat{\theta}^* - \theta_0}{\hat{se}} \right)\right)
$$

---
## Applied example

Let's try it out with the mcr data. 

--

Recall that our $\hat{\theta}$ is the mean of `call_back`.


```{r}
theta_hat
```

--

And our $\hat{se}$ is our estimate of the standard error of the mean, 
$$\hat{se} = \sqrt{\hat{\textrm{Var}}[X]/n};$$ 

```{r}
se_hat
```


---

Suppose we frame our null in terms of the average call back rate among black respondents being 16%--this is the average across all of our respondents.  

$$
H_0: \tau = 0.16 
$$

--

First, our t-statistic under the null is:
$$
\frac{\hat{\theta}^* - \theta_0}{\hat{se}}
$$

```{r}
(tstat <- (theta_hat - 0.16)/se_hat)
```



---


We will consider several different alternative hypotheses. 

---

First, we'll consider the one-tailed alternative hypothesis that $\tau$ is less than zero


$$
H_A: \tau \leq 0
$$

--
Then we calculate our one-sided lower p-value as

$$
p = \Phi\left( \frac{\hat{\theta}^* - \theta_0}{\hat{se}} \right)
$$

Recall that the $p$-value is calculated *under the assumption that the null is true*, so $\theta_0$ is the value of $\theta$ under the null. 

```{r}

(lower_p_value <- pnorm( tstat))



```

--

How do we interpret this?

--

(the probability *under the null distribution* that we would see an estimate of $\hat{\theta}$ that is less than or equal to what we saw from the data.)


---


```{r, fig.width = 6, fig.height=5, fig.align = 'center', echo=FALSE}
result_n <- rnorm(n = 10000, mean = 0.16, sd = se_hat)
plotdata <- tibble(
  x = result_n,
  Fx = pnorm(result_n),
  fx = dnorm(result_n)
)

g <- ggplot(plotdata, aes(x = x, y = fx)) +
  geom_line() +
  coord_cartesian(xlim = c(-6, 6),
                  ylim = c(0,0.5)) +
  ggtitle('PDF of Standard Normal Distribution')

g +
  geom_vline(xintercept = tstat, lty = 'dashed', color = 'skyblue') + 
  geom_segment(aes(x = 0, xend = -1, y = 0.2, yend = 0.2), 
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_segment(aes(x = 0, xend = 1, y = 0.2, yend = 0.2), 
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_point(aes(x = 0, y = 0.2), color = 'skyblue') + 
  annotate(geom="text", x = 0.5, y = .19, label = as.character(expression(sigma)), parse = TRUE, color = 'steelblue') + 
  annotate(geom="text", x = -0.5, y = .19, label = as.character(expression(sigma)), parse = TRUE, color = 'steelblue') + 
  annotate(geom="text", x = 0.075, y = .42, label = as.character(expression(mu)), parse = TRUE, color = 'steelblue')

```



---

```{r}

(upper_p_value <- pnorm( tstat, lower.tail = FALSE ))

lm_robust(call_back ~ 1, data = mcr)

```

```{r}
# 
# (two_sided_t_value <- 2*(1 - pt( tstat, dof)))
# 
# 2 * pt(abs(tstat), df = df.residual(model), lower.tail = FALSE)
# 
# 
# 2 * pnorm(abs(tstat), lower.tail = FALSE)
# 
# summary(lm())
# 
# 
# upper_p_value + lower_p_value
# 
# model$df
# 
# model <- lm_robust(call_back ~1, data = mcr %>% filter(black == 1))
# 
# confint.default(lm(call_back ~1, data = mcr %>% filter(black == 1)))
# 
# 
# lm_robust(call_back ~ 1, data = mcr)

```






## The duality of confidence intervals and hypothesis testing


---

## power of test statistic

---

# Properties of estimators

## Unbiasedness

## Consistency

## MSE

